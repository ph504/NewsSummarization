{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NBKYup-D69GsqcZqiJ37oWuAqrH1LkVp","timestamp":1701366570931}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **BASELINE 1: TextRank**\n","\n","TextRank is an algorithm based on PageRank.\n","The top sentence (most important) from a piece of text will be extracted using this algorithm.\n","\n","Main idea: Fetch the most relevant sentences from the text."],"metadata":{"id":"S9wJQAyBwWaB"}},{"cell_type":"code","source":["#import all the libraries\n","import pandas as pd\n","import re\n","import nltk\n","nltk.download('punkt') #one time download\n","import numpy as np\n","from nltk import sent_tokenize, word_tokenize\n","from nltk.cluster.util import cosine_distance\n","\n","MULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N68ncLrJHav4","executionInfo":{"status":"ok","timestamp":1701366071022,"user_tz":240,"elapsed":3291,"user":{"displayName":"Simin Shehbaz","userId":"11508920856034596911"}},"outputId":"8c47e540-baec-48e8-c400-23a47404a4bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","source":["**STEP 1: LOAD THE DATA**\n","\n","The datasets are pulled from Kaggle, but for ease of use, we published on the web so that they can be used on any system in case the kaggle link changes. The url has been shared when reading the file.\n","\n","```\n","# This is formatted as code\n","```\n","\n"],"metadata":{"id":"6mLzHeRJ5RBZ"}},{"cell_type":"code","source":["#DATASET\n","#reading the data files, published both to the web for ease of access\n","summary = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRwR2KEtXYWDz9_dMrdQZv7Sa-8Vr0pTiRdedvX2A8CY_vuoHUGoarfaFV179puVPbbmKvYaa5ghgh4/pub?gid=1041903722&single=true&output=csv\",\n","                      encoding='iso-8859-1')#news_summary csv file\n","raw = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSFbcrx8oO-z-s-n5_HvlE15BggLWMTX85L2J79FpQ2EvzQgxdaSI69xJolaJP6fbYJDhemQxzRuv23/pub?gid=903434167&single=true&output=csv',\n","                  encoding='iso-8859-1')#news_summary_more csv file"],"metadata":{"id":"rzjJOMa2Hijw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Gi3WOUXrWBEG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, since we have 2 data files, both are combined. The 'text' and 'summary' columns are combined and added to a third dataset, on which the algorithm will conduct analysis."],"metadata":{"id":"5hnsb6EZ5qT5"}},{"cell_type":"code","source":["#combining the data from both files\n","pre1 = raw.iloc[:, 0:2].copy()\n","pre2 = summary.iloc[:, 0:6].copy()\n","\n","# To increase the intake of possible text values to build a reliable model\n","#new column 'text' is created, where all the clomuns except headlines are concatenated under 'text'\n","pre2['text'] = pre2['author'].str.cat(pre2['date'\n","        ].str.cat(pre2['read_more'].str.cat(pre2['text'\n","        ].str.cat(pre2['ctext'], sep=' '), sep=' '), sep=' '), sep=' ')\n","\n","#new table called pre with two columns 'text' and 'summary' acquired from pre1 and pre2\n","df = pd.DataFrame()\n","df['article_text'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\n","df['given_summary'] = pd.concat([pre1['headlines'], pre2['headlines']],\n","                           ignore_index=True)\n","df['predicted_summary']=\"\""],"metadata":{"id":"VMOr0TXsH7Mv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**STEP 2: CLEAN THE DATA**"],"metadata":{"id":"AL-jq4Lt594A"}},{"cell_type":"code","source":["def normalize_whitespace(text):\n","    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n","\n","def _replace_whitespace(match):\n","    text = match.group()\n","    if \"\\n\" in text or \"\\r\" in text:\n","        return \"\\n\"\n","    else:\n","        return \" \"\n","\n","def is_blank(string):\n","    return not string or string.isspace()"],"metadata":{"id":"FBjcT8sG6LU6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**STEP 3: BUILD A SIMILAIRTY MATRIX**\n","\n","This algorithm uses cosine similarity to understand similairty between the sentences. This will then be used to measure distance.\n","So we use this method to measure similarity between all the sentences."],"metadata":{"id":"5dvKjIHM6z1A"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwHHYs28EiG0"},"outputs":[],"source":["def get_symmetric_matrix(matrix):\n","    return matrix + matrix.T - np.diag(matrix.diagonal())\n","\n","def core_cosine_similarity(vector1, vector2):\n","    return 1 - cosine_distance(vector1, vector2)\n","\n","\n","class TextRank():\n","    def __init__(self):\n","        self.damping = 0.85  # damping coefficient, usually is .85\n","        self.min_diff = 1e-5  # convergence threshold\n","        self.steps = 100  # iteration steps\n","        self.text_str = None\n","        self.sentences = None\n","        self.pr_vector = None\n","\n","    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n","        if stopwords is None:\n","            stopwords = []\n","\n","        sent1 = [w.lower() for w in sent1]\n","        sent2 = [w.lower() for w in sent2]\n","\n","        all_words = list(set(sent1 + sent2))\n","\n","        vector1 = [0] * len(all_words)\n","        vector2 = [0] * len(all_words)\n","\n","        # build the vector for the first sentence\n","        for w in sent1:\n","            if w in stopwords:\n","                continue\n","            vector1[all_words.index(w)] += 1\n","\n","        # build the vector for the second sentence\n","        for w in sent2:\n","            if w in stopwords:\n","                continue\n","            vector2[all_words.index(w)] += 1\n","\n","        return core_cosine_similarity(vector1, vector2)\n","\n","    def _build_similarity_matrix(self, sentences, stopwords=None):\n","        # create an empty similarity matrix\n","        sm = np.zeros([len(sentences), len(sentences)])\n","\n","        for idx1 in range(len(sentences)):\n","            for idx2 in range(len(sentences)):\n","                if idx1 == idx2:\n","                    continue\n","                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n","\n","        # Get Symmeric matrix\n","        sm = get_symmetric_matrix(sm)\n","\n","        # Normalize matrix by column\n","        norm = np.sum(sm, axis=0)\n","        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is ignore the 0 element in norm\n","        return sm_norm\n","\n","    def _run_page_rank(self, similarity_matrix):\n","        pr_vector = np.array([1] * len(similarity_matrix))\n","        # Iteration\n","        previous_pr = 0\n","        for epoch in range(self.steps):\n","            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n","            if abs(previous_pr - sum(pr_vector)) < self.min_diff:\n","                break\n","            else:\n","                previous_pr = sum(pr_vector)\n","        return pr_vector\n","\n","    def _get_sentence(self, index):\n","        try:\n","            return self.sentences[index]\n","        except IndexError:\n","            return \"\"\n","\n","    def get_top_sentences(self, number=5):\n","        top_sentences = []\n","        if self.pr_vector is not None:\n","            sorted_pr = np.argsort(self.pr_vector)\n","            sorted_pr = list(sorted_pr)\n","            sorted_pr.reverse()\n","            index = 0\n","            for epoch in range(number):\n","                sent = self.sentences[sorted_pr[index]]\n","                sent = normalize_whitespace(sent)\n","                top_sentences.append(sent)\n","                index += 1\n","        return top_sentences\n","\n","    def analyze(self, text, stop_words=None):\n","        self.text_str = text\n","        self.sentences = sent_tokenize(self.text_str)\n","        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n","        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n","        self.pr_vector = self._run_page_rank(similarity_matrix)"]},{"cell_type":"code","source":["length = len(df['article_text'])\n","print (length) #using this to run a for loop to get summary for each article"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkJwsSBIHVQR","executionInfo":{"status":"ok","timestamp":1701366105970,"user_tz":240,"elapsed":164,"user":{"displayName":"Simin Shehbaz","userId":"11508920856034596911"}},"outputId":"f5e31bac-67ea-49b2-e258-74d1a087d108"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["102915\n"]}]},{"cell_type":"markdown","source":["**STEP 4: RUN THE TEXTRANK ALGORITHM**\n","\n","Now, we call the text rank algorithm for each part sentence and store the most important sentence in a new column of the data file called \"predicted summary\" - this is essentially what the TextRank is using as a summarizer."],"metadata":{"id":"3x9EHCHz7jSF"}},{"cell_type":"code","source":["i=0\n","while i<length:\n","  news_txt = str(df['article_text'][i]) #convert the article to string\n","  summary_obj = TextRank()\n","  summary_obj.analyze(news_txt)\n","  summary_txt=summary_obj.get_top_sentences(1)\n","  df['predicted_summary'][i]=summary_txt\n","  i = i+1"],"metadata":{"id":"MG0XyL-uIkgv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**STEP 5: CALCULATE THE ROUGE SCORE**\n","\n"],"metadata":{"id":"om2lK68k8AwH"}},{"cell_type":"code","source":["#ROUGE SCORE\n","!pip install rouge\n","!pip install rouge_score\n","!pip install evaluate"],"metadata":{"id":"mn67SlIyU4_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from rouge import Rouge\n","\n","# Initialize\n","rouge = Rouge()\n","\n","#convert the data cells into strings and put them in a list\n","golden_summaries = df['given_summary'].astype(str).tolist()\n","predicted_summaries = df['predicted_summary'].astype(str).tolist()\n","\n","# Calaculate ROUGE scores\n","scores = rouge.get_scores(predicted_summaries, golden_summaries, avg=True)\n","print (scores)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RknR1NPCYM7h","executionInfo":{"status":"ok","timestamp":1701217361401,"user_tz":240,"elapsed":38203,"user":{"displayName":"Simin Shehbaz","userId":"11508920856034596911"}},"outputId":"0e1657ec-ad8a-45f4-a113-3519071396f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'rouge-1': {'r': 0.27756708945018227, 'p': 0.11796380760258501, 'f': 0.16212791732320814}, 'rouge-2': {'r': 0.08367072823639744, 'p': 0.03126422381280783, 'f': 0.04461420200277103}, 'rouge-l': {'r': 0.241761188306146, 'p': 0.10307887986748608, 'f': 0.14147646896098073}}\n"]}]},{"cell_type":"code","source":["golden_summaries(1)"],"metadata":{"id":"mXjvUb-uzomo","executionInfo":{"status":"error","timestamp":1701323919039,"user_tz":240,"elapsed":9,"user":{"displayName":"Simin Shehbaz","userId":"11508920856034596911"}},"outputId":"55c1515d-9010-4c2d-99aa-f62eae658a1f","colab":{"base_uri":"https://localhost:8080/","height":176}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3c833188036f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgolden_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'golden_summaries' is not defined"]}]}]}